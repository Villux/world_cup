\section{Predicting football match outcome with Random Forest Classifier}
What is is done. Why random forest? What are the alternative methods
\subsection{Random Forest}
\label{ss:randomforest}
In machine learning supervised learning is the task of learning the relationship between input features and and the target value. Structure that describes this relathionship is called a model. In most cases these models are used to predict the target value based on new input features. There are two types of models: \textit{regression models} and \textit{classifier models}. If the target value is in a real-valued domain the model is called \textit{regression model}. \textit{Classifier models} are used to map the input features to predefined classes. \cite{rokach2005top}

Decision tree is one of the most popular model type used in classification problems. Decision tree is a directed tree, which means that all of the nodes, except the \textit{root node}, have exactly one incoming edge. Nodes that have outgoing edges are called \textit{internal nodes} and the nodes that have only incoming edge are called the \textit{leave nodes}. Internal nodes in decision tree split the instance space into two or more subspaces according to a certain discrete function of the input's feature values. Usually split is done based on a single feature from the whole feature vector. A single class value is assigned for the \textit{leave nodes}. When new input is given tree is navigated from the \textit{root node} to a \textit{leave node} which determinates the predicted class label. In regression these target values can take continuous values. \cite{rokach2005top}

Decision trees have many benefits and are very useful "off-the-shelf" predictors. Outliers in the dataset or many irrelevant predictors are not problematic for the trees. Scaling or any other general transformation can be done to the input space since trees are invariant under transformation of the individual predictors. \cite{friedman2001elements} Decision trees have good interpretability if the trees are small.

On main disadvantage of the decision trees is bad prediction accuracy \cite{friedman2001elements}. Decision trees can express the training data well, but have high variance which means that prediction accuracy for unseen data is often worse compared to other models.

Bootstrap aggragating, also called bagging, is a way to improve the prediction accuracy of decision trees by averaging. In bagging the  average is taken over the output of a multiple estimators:
\begin{align}
    \hat {f_{bag}}(x) = \frac{1}{B}\sum_{b = 1}^{B} \hat {f}^{*b}(x)
\end{align}
where B is the number of estimators and ${f}^{*}$ is a single estimator. This reduces the high variance of a single tree and makes predictions more accurate.

Bootstrap in bagging means that in the training of a single tree a random sample with replacement is taken from the original sample. Samples used in training come from the same distribution, meaning that the trees are identically distributed (i.d.). This combined with deep trees that have less bias ensures that the variance reduction achieved in bagging comes with a expense of small increase in bias and the loss of interpretability. The loss of interpretability can't be avoided since a single tree can't be used anymore for reasoning. Trees in bagging are only identically distributed. The missing independent property means that the trees in the forest can have pairwise correlation. This is common in cases where input data has one strong predictor which often leads to a situation where all of the tree are splitted similarily. \cite{friedman2001elements}

Amit and Geman's \cite{amit1997shape} idea of random feature selection inspired Breiman to use bagging in tandem with random feature selection. With this random feature selection correlation between the trees can be reduced since the generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them \cite{breiman2001random}. Breiman was first to use the name \textit{Random Forest} for algorithms that use bagging and random feature selection with tree predictors \cite{breiman2001random}. Step-by-step instruction from \cite{friedman2001elements} for Random Forest algorithm are listed in the Algorithm \ref{alg:random_forest}.

Main usecases for Random forest are \textit{classification} and \textit{regression}.

\begin{algorithm}
    \footnotesize
    \begin{minipage}{.92\linewidth}
    Here I have some text that I need to have within the algorithm that
    sets up the problem, and the text stretches over two  lines.
    \textbf{Note that the second line has a weird indentation.}
    \BlankLine
    For all the of the seconds in the day:
    \begin{enumerate}
        \item For $b = 1$ to $B$:
        \begin{enumerate}
            \item Draw a bootstrap sample $\bm{Z}^{*}$ of size $N$ from the training data.
            \item Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached.
            \begin{enumerate}
                \item Select $m$ variables at random from the $p$ variables.
                \item Pick the best variable/split-point among the $m$.
                \item Split the node into two daughter nodes.
            \end{enumerate}
        \end{enumerate}
        \item Output the ensemble of trees $\left\{ T _ { b } \right\} _ { 1 } ^ { B }$.
    \end{enumerate}
    To make a prediction at a new point $x$:

    \textit{Regression:} $\hat { f } _ { \mathrm { rf } } ^ { B } ( x ) = \frac { 1 } { B } \sum _ { b = 1 } ^ { B } T _ { b } ( x )$

    \textit{Classification:} Let $\hat { C } _ { b } ( x )$ be the class prediction of the $b$th random forest
    tree. Then $\hat{C} _ { r f } ^ { B } ( x )$ = \textit{majority vote} $\left\{ \hat { C } _ { b } ( x ) \right\} _ { 1 } ^ { B }$
    \end{minipage}
    \caption{\footnotesize Random Forest for Regression or Classification.}
    \label{alg:random_forest}
\end{algorithm}
\subsection{Random forest hyperparameter selection}
Random forest, as implemented in most of the programming libraries, can be used out-of-the-box with fairly good perfomance. Reason for this is that the default values for certain hyperparameters work well in most of the cases. Lot of effort has been used to study the effects of hyperparamater selection in random forest. Main idea in hyperparameter selection with random forest is to keep the correlation between the trees small while maintaining reasonable strength. In this section I will share some light to this process of hyperparameter selection and introduce an algorithm that will be used to select the most optimal hyperparamters for the prediction model.

One key hyperparameter in random forest is the number of randomly drawn candidate predictors, denoted as $K$. As mentioned in \ref{ss:randomforest} this part of random forest algorithm is critical if it's important to achieve low correlation between the trees in the forest. Bernard et al. \cite{bernard2009influence} investigated Breiman's Forest-RI algorithm's performance with different K values using several datasets. Their conclusion was that $K=\sqrt(M)$, where $M$ is the predictor space, is a reasonable setting to induct near-optimal random forest. In cases where there are many or only few relevant predictor variables, choosing the K can have high influence on the results. For example in the case of minuscule $K$ with a dataset that has only small number of important predictors most of the trees are built without the important predictor and have low prediction accuracy. \cite{bernard2009influence} Most of the time while running the algorithm is used to choose the split variables. When number of candidate variables is decreased computation time approximately decreases linearly. \cite{wright2017unbiased}

Another hyperparameter that has the correlation lowering effect is the subsample size. Typically the subsample size is the same as the dataset's size, even when subset is formed with replacements. In their emprical analysis Martinez et al. \cite{martinez2010out} concluded that selecting the subsample size with out-of-bag error can increase the performance of the model and the correct value for subsample size hyperparameter is problem dependent.

Two important hyperparameter related to node splitting are node size and splitting criterion. Segal \cite{segal2004machine} showed that increasing the amount of noise variables lead to higher optimal node size. For splitting criterion most of the software packages use gini impurity for classification and MSE for regression, which were suggested by Breiman \cite{breiman2001random}. Probst et al. \cite{probst2018hyperparameters} concluded from multiple studies that not a single criterion has been proven superior to the others regarding the predictive performance.

Since random forest is an ensemble method, the number of estimators (trees) can be changed. For this hyperparameter it's important to set the value as high enough, since after a certain point the variance can't be lowered anymore and the computation time used in training grows beyond resonable \cite{probst2018hyperparameters}.

\subsection{Variable importance in Random Forest}
\subsection{Model specifications}
\section{Forecasting procedure and measuments of model's forecasting accuracy}
\subsection{Description of our forecasting procedure}
\subsection{Forecast accuracy measurements}
Precision, recall, F-score, accuracy of probabilistic prediction: Brier score, label ranking loss
\subsection{Evaluating betting strategy}
UNIT, KELLY
% https://ac-els-cdn-com.libproxy.aalto.fi/S0169207009001708/1-s2.0-S0169207009001708-main.pdf?_tid=ad56a7bb-de9d-4230-bcde-1359866f7b5d&acdnat=1531135889_11cc5428173cb5f67c64d3e19d9f1ae3
% Stopping rule? https://search-proquest-com.libproxy.aalto.fi/docview/1986130682?pq-origsite=gscholar




