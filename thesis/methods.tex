\section{Predicting football match outcome with Random Forest Classifier}
What is is done. Why random forest? What are the alternative methods
\subsection{Random Forest}
Decision tree
Overfitting, high variance, bad prediction power, CART

\subsubsection{bagging}
The problem of high variance in decision trees is problematic. Bootstrap aggragating, also called bagging, is a way to reduce this high variance of decision trees by averaging. Averiging is done over multiple estimators:
\begin{align}
    \hat {f_{bag}}(x) = \frac{1}{B}\sum_{b = 1}^{B} \hat {f}^{*b}(x)
\end{align}
where B is the number of estimators and ${f}^{*}$ is a single estimator. Bootstrap in bagging means that in the training of a single tree a random sample with replacement is taken from the original sample. Samples used in training come from the same distribution which means that the trees are identically distributed (i.d.). This combined with deep trees that have less bias means that the variance reduction achieved in bagging comes with a expense of small increase in bias and the loss of interpretability. A single tree can't be used anymore for reasoning. Since trees in bagging are not independent trees can have high correlation betweem each other. This is common in cases where we have one strong predictor which often means that each tree is splitted similarily. \cite{friedman2001elements}

Amit and Geman's \cite{amit1997shape} idea of random feature selection that inspired Breiman to use bagging in tandem with random feature selection which he named as Random Forest \cite{breiman2001random}. With this random feature selection correlation between the trees can reduced since the generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them \cite{breiman2001random}.

random subsampling
Variance reduction more, higher feature space

random forest
Low variance, high prediction power, if grown sufficiently deep, have relatively low bias
Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the ex- pectation of any one of them.
\subsection{Random forest hyperparameter selection}
\subsection{Variable importance in Random Forest}
\subsection{Model specifications}
\section{Forecasting procedure and measuments of model's forecasting accuracy}
\subsection{Description of our forecasting procedure}
\subsection{Forecast accuracy measurements}
Precision, recall, F-score, accuracy of probabilistic prediction: Brier score, label ranking loss
\subsection{Evaluating betting strategy}
UNIT, KELLY
% https://ac-els-cdn-com.libproxy.aalto.fi/S0169207009001708/1-s2.0-S0169207009001708-main.pdf?_tid=ad56a7bb-de9d-4230-bcde-1359866f7b5d&acdnat=1531135889_11cc5428173cb5f67c64d3e19d9f1ae3
% Stopping rule? https://search-proquest-com.libproxy.aalto.fi/docview/1986130682?pq-origsite=gscholar




